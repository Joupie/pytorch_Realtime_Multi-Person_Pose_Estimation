{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPiJMPWoA5XjSe+QyR5pvVv",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Joupie/pytorch_Realtime_Multi-Person_Pose_Estimation/blob/master/webcam_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xzo4qUOvi1uy",
        "outputId": "74cf854f-3ede-449d-918a-60ecaec6b35b"
      },
      "source": [
        "!wget https://www.dropbox.com/s/ae071mfm2qoyc8v/pose_model.pth"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-15 19:29:38--  https://www.dropbox.com/s/ae071mfm2qoyc8v/pose_model.pth\n",
            "Resolving www.dropbox.com (www.dropbox.com)... 162.125.5.18, 2620:100:601d:18::a27d:512\n",
            "Connecting to www.dropbox.com (www.dropbox.com)|162.125.5.18|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: /s/raw/ae071mfm2qoyc8v/pose_model.pth [following]\n",
            "--2021-11-15 19:29:39--  https://www.dropbox.com/s/raw/ae071mfm2qoyc8v/pose_model.pth\n",
            "Reusing existing connection to www.dropbox.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://ucd39f1a99e151b8b4c314038de9.dl.dropboxusercontent.com/cd/0/inline/BaC8RmwpcBSi2gKbKjPIEPcZ8KuT6UmInA4k97x5nICFuD_gvmd13byT75VZ7UoiauERZ9wnOI3H-w9Me26GHSQLxG2soWjM4Q-_D2cfoiNLgRWoiDrBiZvRjP9vihRqd-giX4QRXj75_HkdQNhqCzOb/file# [following]\n",
            "--2021-11-15 19:29:39--  https://ucd39f1a99e151b8b4c314038de9.dl.dropboxusercontent.com/cd/0/inline/BaC8RmwpcBSi2gKbKjPIEPcZ8KuT6UmInA4k97x5nICFuD_gvmd13byT75VZ7UoiauERZ9wnOI3H-w9Me26GHSQLxG2soWjM4Q-_D2cfoiNLgRWoiDrBiZvRjP9vihRqd-giX4QRXj75_HkdQNhqCzOb/file\n",
            "Resolving ucd39f1a99e151b8b4c314038de9.dl.dropboxusercontent.com (ucd39f1a99e151b8b4c314038de9.dl.dropboxusercontent.com)... 162.125.5.15, 2620:100:601d:15::a27d:50f\n",
            "Connecting to ucd39f1a99e151b8b4c314038de9.dl.dropboxusercontent.com (ucd39f1a99e151b8b4c314038de9.dl.dropboxusercontent.com)|162.125.5.15|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: /cd/0/inline2/BaCsKFDEX14CdKHKPK14RAKWovxpp1RIV36d1JZ8IlHCbqZJwKUfalYzeXt5cXw3d0IQT6PDUfCjaT5QTCqAvn001O7_lSBxfY48t4hDzV6TVYC8O3dxE8LiNWNXmcr0Abtj_8_p6q5sQeycBKhYupVnLm9k4og5nYiDxjG-KTMSs5jq6LSkgSLA2YP3LBT9r-w84Hgc-XnZr0sPdt-a1T6Upwg5UbC2vYMKyPaIJCKUlFu4zuPBfHqTWZEnbbV_MwnwpelwLcdYDotQOM4ucljQZ6l0B2I4RhyjLfy4IjUYGS1UiseNeByR0beibMGDBozKF-RdoSW6YrozVA8PheC-0wTTmI1zr5bWc7Uszv7kHp0-FIbQzLX5SvAPDJAkncc/file [following]\n",
            "--2021-11-15 19:29:40--  https://ucd39f1a99e151b8b4c314038de9.dl.dropboxusercontent.com/cd/0/inline2/BaCsKFDEX14CdKHKPK14RAKWovxpp1RIV36d1JZ8IlHCbqZJwKUfalYzeXt5cXw3d0IQT6PDUfCjaT5QTCqAvn001O7_lSBxfY48t4hDzV6TVYC8O3dxE8LiNWNXmcr0Abtj_8_p6q5sQeycBKhYupVnLm9k4og5nYiDxjG-KTMSs5jq6LSkgSLA2YP3LBT9r-w84Hgc-XnZr0sPdt-a1T6Upwg5UbC2vYMKyPaIJCKUlFu4zuPBfHqTWZEnbbV_MwnwpelwLcdYDotQOM4ucljQZ6l0B2I4RhyjLfy4IjUYGS1UiseNeByR0beibMGDBozKF-RdoSW6YrozVA8PheC-0wTTmI1zr5bWc7Uszv7kHp0-FIbQzLX5SvAPDJAkncc/file\n",
            "Reusing existing connection to ucd39f1a99e151b8b4c314038de9.dl.dropboxusercontent.com:443.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 209266467 (200M) [application/octet-stream]\n",
            "Saving to: ‘pose_model.pth’\n",
            "\n",
            "pose_model.pth      100%[===================>] 199.57M  58.8MB/s    in 3.6s    \n",
            "\n",
            "2021-11-15 19:29:44 (55.1 MB/s) - ‘pose_model.pth’ saved [209266467/209266467]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OyUyMwR6kQRm",
        "outputId": "920194c6-1c09-4da7-ab92-c629f74d5ea4"
      },
      "source": [
        "!apt-get install python python-setuptools python-dev python-augeas gcc swig dialog"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "python is already the newest version (2.7.15~rc1-1).\n",
            "python-dev is already the newest version (2.7.15~rc1-1).\n",
            "gcc is already the newest version (4:7.4.0-1ubuntu2.3).\n",
            "gcc set to manually installed.\n",
            "The following additional packages will be installed:\n",
            "  augeas-lenses libaugeas0 python-pkg-resources swig3.0\n",
            "Suggested packages:\n",
            "  augeas-doc augeas-tools python-setuptools-doc swig-doc swig-examples\n",
            "  swig3.0-examples swig3.0-doc\n",
            "The following NEW packages will be installed:\n",
            "  augeas-lenses dialog libaugeas0 python-augeas python-pkg-resources\n",
            "  python-setuptools swig swig3.0\n",
            "0 upgraded, 8 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 2,243 kB of archives.\n",
            "After this operation, 11.3 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 augeas-lenses all 1.10.1-2ubuntu1 [300 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 dialog amd64 1.3-20171209-1 [217 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libaugeas0 amd64 1.10.1-2ubuntu1 [160 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-augeas all 0.5.0-1 [9,204 B]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-pkg-resources all 39.0.1-2 [128 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic/main amd64 python-setuptools all 39.0.1-2 [329 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig3.0 amd64 3.0.12-1 [1,094 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu bionic/universe amd64 swig amd64 3.0.12-1 [6,460 B]\n",
            "Fetched 2,243 kB in 1s (2,281 kB/s)\n",
            "Selecting previously unselected package augeas-lenses.\n",
            "(Reading database ... 155219 files and directories currently installed.)\n",
            "Preparing to unpack .../0-augeas-lenses_1.10.1-2ubuntu1_all.deb ...\n",
            "Unpacking augeas-lenses (1.10.1-2ubuntu1) ...\n",
            "Selecting previously unselected package dialog.\n",
            "Preparing to unpack .../1-dialog_1.3-20171209-1_amd64.deb ...\n",
            "Unpacking dialog (1.3-20171209-1) ...\n",
            "Selecting previously unselected package libaugeas0:amd64.\n",
            "Preparing to unpack .../2-libaugeas0_1.10.1-2ubuntu1_amd64.deb ...\n",
            "Unpacking libaugeas0:amd64 (1.10.1-2ubuntu1) ...\n",
            "Selecting previously unselected package python-augeas.\n",
            "Preparing to unpack .../3-python-augeas_0.5.0-1_all.deb ...\n",
            "Unpacking python-augeas (0.5.0-1) ...\n",
            "Selecting previously unselected package python-pkg-resources.\n",
            "Preparing to unpack .../4-python-pkg-resources_39.0.1-2_all.deb ...\n",
            "Unpacking python-pkg-resources (39.0.1-2) ...\n",
            "Selecting previously unselected package python-setuptools.\n",
            "Preparing to unpack .../5-python-setuptools_39.0.1-2_all.deb ...\n",
            "Unpacking python-setuptools (39.0.1-2) ...\n",
            "Selecting previously unselected package swig3.0.\n",
            "Preparing to unpack .../6-swig3.0_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig3.0 (3.0.12-1) ...\n",
            "Selecting previously unselected package swig.\n",
            "Preparing to unpack .../7-swig_3.0.12-1_amd64.deb ...\n",
            "Unpacking swig (3.0.12-1) ...\n",
            "Setting up swig3.0 (3.0.12-1) ...\n",
            "Setting up dialog (1.3-20171209-1) ...\n",
            "Setting up python-pkg-resources (39.0.1-2) ...\n",
            "Setting up augeas-lenses (1.10.1-2ubuntu1) ...\n",
            "Setting up python-setuptools (39.0.1-2) ...\n",
            "Setting up swig (3.0.12-1) ...\n",
            "Setting up libaugeas0:amd64 (1.10.1-2ubuntu1) ...\n",
            "Setting up python-augeas (0.5.0-1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_l8BItRsjGif",
        "outputId": "3972aaff-9e05-49b9-daac-c94fee4c953f"
      },
      "source": [
        "!git clone https://github.com/Joupie/pytorch_Realtime_Multi-Person_Pose_Estimation.git\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch_Realtime_Multi-Person_Pose_Estimation'...\n",
            "remote: Enumerating objects: 110, done.\u001b[K\n",
            "remote: Counting objects: 100% (42/42), done.\u001b[K\n",
            "remote: Compressing objects: 100% (32/32), done.\u001b[K\n",
            "remote: Total 110 (delta 12), reused 29 (delta 10), pack-reused 68\u001b[K\n",
            "Receiving objects: 100% (110/110), 54.36 MiB | 34.15 MiB/s, done.\n",
            "Resolving deltas: 100% (16/16), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TRq9GE1Xm2Ap",
        "outputId": "a38b1060-d4a3-4599-cbc8-64aea9a2e986"
      },
      "source": [
        "!pip install -r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from -r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 1)) (0.29.24)\n",
            "Requirement already satisfied: torch>=1.2 in /usr/local/lib/python3.7/dist-packages (from -r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 2)) (1.10.0+cu111)\n",
            "Requirement already satisfied: torchvision>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from -r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 3)) (0.11.1+cu111)\n",
            "Collecting progress\n",
            "  Downloading progress-1.6.tar.gz (7.8 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from -r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 5)) (3.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from -r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: pycocotools in /usr/local/lib/python3.7/dist-packages (from -r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 7)) (2.0.2)\n",
            "Collecting yacs\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.2->-r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 2)) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->-r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 3)) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.4.0->-r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 3)) (7.1.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 5)) (1.3.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 5)) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 5)) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->-r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 5)) (0.11.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib->-r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 5)) (1.15.0)\n",
            "Requirement already satisfied: setuptools>=18.0 in /usr/local/lib/python3.7/dist-packages (from pycocotools->-r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 7)) (57.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from yacs->-r /content/pytorch_Realtime_Multi-Person_Pose_Estimation/requirements.txt (line 8)) (3.13)\n",
            "Building wheels for collected packages: progress\n",
            "  Building wheel for progress (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for progress: filename=progress-1.6-py3-none-any.whl size=9628 sha256=a672ba1412c950dfa967f447d1e2d34bb8ad992fd1e78f6a336699fb33b5fc60\n",
            "  Stored in directory: /root/.cache/pip/wheels/8e/d7/61/498d8e27dc11e9805b01eb3539e2ee344436fc226daeb5fe87\n",
            "Successfully built progress\n",
            "Installing collected packages: yacs, progress\n",
            "Successfully installed progress-1.6 yacs-0.1.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dtdE4BHdOrQ5",
        "outputId": "965d5efb-2a7f-4f83-c282-824dbb95c443"
      },
      "source": [
        "\n",
        "%cd /content/pytorch_Realtime_Multi-Person_Pose_Estimation"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch_Realtime_Multi-Person_Pose_Estimation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4v9L9hrOyaO",
        "outputId": "87d1206c-788f-4cd6-d743-c3170fdbe056"
      },
      "source": [
        "\n",
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/pytorch_Realtime_Multi-Person_Pose_Estimation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCCZbyYxO_hk",
        "outputId": "f4ec69ed-7882-47ec-86df-a4d7e5617505"
      },
      "source": [
        "\n",
        "!git remote -v"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "origin\thttps://github.com/Joupie/pytorch_Realtime_Multi-Person_Pose_Estimation.git (fetch)\n",
            "origin\thttps://github.com/Joupie/pytorch_Realtime_Multi-Person_Pose_Estimation.git (push)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FhjCq0aSPG98",
        "outputId": "a7a64484-c184-4439-e86e-0b341a0945c7"
      },
      "source": [
        "\n",
        "!git status"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Your branch is up to date with 'origin/master'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git reset HEAD <file>...\" to unstage)\n",
            "\n",
            "\t\u001b[32mmodified:   demo/web_demo.py\u001b[m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BZVGEN3PlWJ"
      },
      "source": [
        "\n",
        "!git add -A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEhudLTKPzK_",
        "outputId": "b3e4d572-88d5-49d4-83c7-8166f6c5880f"
      },
      "source": [
        "\n",
        "!git commit -a -m \"first commit\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "*** Please tell me who you are.\n",
            "\n",
            "Run\n",
            "\n",
            "  git config --global user.email \"you@example.com\"\n",
            "  git config --global user.name \"Your Name\"\n",
            "\n",
            "to set your account's default identity.\n",
            "Omit --global to set the identity only in this repository.\n",
            "\n",
            "fatal: unable to auto-detect email address (got 'root@b3fca20cbc08.(none)')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXPERQZ5QO-v"
      },
      "source": [
        "\n",
        "!git config --global user.email \"joupining@gmail.com\"\n",
        "!git config --global user.name \"Joupie\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgquG0SNQS9s",
        "outputId": "0e301c5b-07ab-4cd6-a406-6828eee5c8b3"
      },
      "source": [
        "\n",
        "!git config --list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "user.email=joupining@gmail.com\n",
            "user.name=Joupie\n",
            "core.repositoryformatversion=0\n",
            "core.filemode=true\n",
            "core.bare=false\n",
            "core.logallrefupdates=true\n",
            "remote.origin.url=https://github.com/Joupie/pytorch_Realtime_Multi-Person_Pose_Estimation.git\n",
            "remote.origin.fetch=+refs/heads/*:refs/remotes/origin/*\n",
            "branch.master.remote=origin\n",
            "branch.master.merge=refs/heads/master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zvZhglFYQe5n",
        "outputId": "f3bf7be1-6be5-41d3-8d09-cc45497b35d8"
      },
      "source": [
        "\n",
        "!git commit -a -m \"first commit\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[master f6b4c9e] first commit\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYqQNa-5Qli0",
        "outputId": "dea26ade-9abc-48da-fed1-fc3a6e93703a"
      },
      "source": [
        "\n",
        "!git status"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "On branch master\n",
            "Your branch is ahead of 'origin/master' by 1 commit.\n",
            "  (use \"git push\" to publish your local commits)\n",
            "\n",
            "nothing to commit, working tree clean\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0XyH2zEQ84C",
        "outputId": "1ebdc708-08b0-4533-e9a3-1a0e37b99c35"
      },
      "source": [
        "\n",
        "username = input(\"username : \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "username : Joupie\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MbjwtfBhRPmO",
        "outputId": "0595b925-3301-43f9-9c1f-f14781d00bd5"
      },
      "source": [
        "\n",
        "from getpass import getpass\n",
        "\n",
        "password = getpass(\"pass : \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "pass : ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGJjQ9xLRs_U"
      },
      "source": [
        "\n",
        "!git remote rm origin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nymyTdhlRcOy"
      },
      "source": [
        "\n",
        "!git remote add origin https://$username:$password@github.com/$username/pytorch_Realtime_Multi-Person_Pose_Estimation.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ah9syWUR0R4",
        "outputId": "74419d24-06b8-4781-c95d-c926b7e3bf13"
      },
      "source": [
        "\n",
        "!git push origin master"
      ],
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counting objects: 4, done.\n",
            "Delta compression using up to 2 threads.\n",
            "Compressing objects:  25% (1/4)   \rCompressing objects:  50% (2/4)   \rCompressing objects:  75% (3/4)   \rCompressing objects: 100% (4/4)   \rCompressing objects: 100% (4/4), done.\n",
            "Writing objects:  25% (1/4)   \rWriting objects:  50% (2/4)   \rWriting objects:  75% (3/4)   \rWriting objects: 100% (4/4)   \rWriting objects: 100% (4/4), 391 bytes | 391.00 KiB/s, done.\n",
            "Total 4 (delta 2), reused 0 (delta 0)\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/Joupie/pytorch_Realtime_Multi-Person_Pose_Estimation.git\n",
            "   b3e8abf..f6b4c9e  master -> master\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JcW9CUe7i7qz",
        "outputId": "89214886-c0d1-4f0a-9865-09cff5a7aa59"
      },
      "source": [
        "%cd /content/pytorch_Realtime_Multi-Person_Pose_Estimation/lib/pafprocess\n",
        "!bash make.sh"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch_Realtime_Multi-Person_Pose_Estimation/lib/pafprocess\n",
            "running build_ext\n",
            "building '_pafprocess' extension\n",
            "swigging pafprocess.i to pafprocess_wrap.cpp\n",
            "swig -python -c++ -o pafprocess_wrap.cpp pafprocess.i\n",
            "creating build\n",
            "creating build/temp.linux-x86_64-3.7\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I. -I/usr/include/python3.7m -c pafprocess.cpp -o build/temp.linux-x86_64-3.7/pafprocess.o\n",
            "x86_64-linux-gnu-gcc -pthread -Wno-unused-result -Wsign-compare -DNDEBUG -g -fwrapv -O2 -Wall -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 -fPIC -I/usr/local/lib/python3.7/dist-packages/numpy/core/include -I. -I/usr/include/python3.7m -c pafprocess_wrap.cpp -o build/temp.linux-x86_64-3.7/pafprocess_wrap.o\n",
            "x86_64-linux-gnu-g++ -pthread -shared -Wl,-O1 -Wl,-Bsymbolic-functions -Wl,-Bsymbolic-functions -Wl,-z,relro -Wl,-Bsymbolic-functions -Wl,-z,relro -g -fdebug-prefix-map=/build/python3.7-Y7dWVB/python3.7-3.7.12=. -fstack-protector-strong -Wformat -Werror=format-security -Wdate-time -D_FORTIFY_SOURCE=2 build/temp.linux-x86_64-3.7/pafprocess.o build/temp.linux-x86_64-3.7/pafprocess_wrap.o -o /content/pytorch_Realtime_Multi-Person_Pose_Estimation/lib/pafprocess/_pafprocess.cpython-37m-x86_64-linux-gnu.so\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "souUk1QpjJE-"
      },
      "source": [
        "%cd /content/pytorch_Realtime_Multi-Person_Pose_Estimation\n",
        "!python /content/pytorch_Realtime_Multi-Person_Pose_Estimation/demo/web_demo.py"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "4_xk3r-AvUMS",
        "outputId": "76185bc1-e070-49d8-c823-e146f7a154d6"
      },
      "source": [
        "from demo.web_demo import model"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--cfg CFG] [--weight WEIGHT] ...\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHTWduvVfpfS",
        "outputId": "0c2de335-536d-4826-9c43-18eb8e5f715f"
      },
      "source": [
        "%cd /content/pytorch_Realtime_Multi-Person_Pose_Estimation"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/pytorch_Realtime_Multi-Person_Pose_Estimation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q_r61BcxjFWJ"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "sys.path.append('.')\n",
        "import cv2\n",
        "import math\n",
        "import time\n",
        "import scipy\n",
        "import argparse\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import pylab as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from collections import OrderedDict\n",
        "from scipy.ndimage.morphology import generate_binary_structure\n",
        "from scipy.ndimage.filters import gaussian_filter, maximum_filter\n",
        "\n",
        "from lib.network.rtpose_vgg import get_model\n",
        "from lib.network import im_transform\n",
        "from lib.config import update_config, cfg\n",
        "from evaluate.coco_eval import get_outputs, handle_paf_and_heat\n",
        "from lib.utils.common import Human, BodyPart, CocoPart, CocoColors, CocoPairsRender, draw_humans\n",
        "from lib.utils.paf_to_pose import paf_to_pose_cpp\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode, b64encode\n",
        "import PIL\n",
        "from IPython.display import display, Javascript, Image\n",
        "from google.colab.output import eval_js\n",
        "from google.colab.patches import cv2_imshow\n",
        "from base64 import b64decode, b64encode\n",
        "import cv2\n",
        "import numpy as np\n",
        "import PIL\n",
        "import io\n",
        "import html\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eRQ4GacjIGa"
      },
      "source": [
        "\n",
        "\n",
        "class Namespace:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.__dict__.update(kwargs)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gHP8X0MTjIAS"
      },
      "source": [
        "# JavaScript to properly create our live video stream using our webcam as input\n",
        "def video_stream():\n",
        "  js = Javascript('''\n",
        "    var video;\n",
        "    var div = null;\n",
        "    var stream;\n",
        "    var captureCanvas;\n",
        "    var imgElement;\n",
        "    var labelElement;\n",
        "    \n",
        "    var pendingResolve = null;\n",
        "    var shutdown = false;\n",
        "    \n",
        "    function removeDom() {\n",
        "       stream.getVideoTracks()[0].stop();\n",
        "       video.remove();\n",
        "       div.remove();\n",
        "       video = null;\n",
        "       div = null;\n",
        "       stream = null;\n",
        "       imgElement = null;\n",
        "       captureCanvas = null;\n",
        "       labelElement = null;\n",
        "    }\n",
        "    \n",
        "    function onAnimationFrame() {\n",
        "      if (!shutdown) {\n",
        "        window.requestAnimationFrame(onAnimationFrame);\n",
        "      }\n",
        "      if (pendingResolve) {\n",
        "        var result = \"\";\n",
        "        if (!shutdown) {\n",
        "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
        "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
        "        }\n",
        "        var lp = pendingResolve;\n",
        "        pendingResolve = null;\n",
        "        lp(result);\n",
        "      }\n",
        "    }\n",
        "    \n",
        "    async function createDom() {\n",
        "      if (div !== null) {\n",
        "        return stream;\n",
        "      }\n",
        "\n",
        "      div = document.createElement('div');\n",
        "      div.style.border = '2px solid black';\n",
        "      div.style.padding = '3px';\n",
        "      div.style.width = '100%';\n",
        "      div.style.maxWidth = '600px';\n",
        "      document.body.appendChild(div);\n",
        "      \n",
        "      const modelOut = document.createElement('div');\n",
        "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
        "      labelElement = document.createElement('span');\n",
        "      labelElement.innerText = 'No data';\n",
        "      labelElement.style.fontWeight = 'bold';\n",
        "      modelOut.appendChild(labelElement);\n",
        "      div.appendChild(modelOut);\n",
        "           \n",
        "      video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      video.width = div.clientWidth - 6;\n",
        "      video.setAttribute('playsinline', '');\n",
        "      video.onclick = () => { shutdown = true; };\n",
        "      stream = await navigator.mediaDevices.getUserMedia(\n",
        "          {video: { facingMode: \"environment\"}});\n",
        "      div.appendChild(video);\n",
        "\n",
        "      imgElement = document.createElement('img');\n",
        "      imgElement.style.position = 'absolute';\n",
        "      imgElement.style.zIndex = 1;\n",
        "      imgElement.onclick = () => { shutdown = true; };\n",
        "      div.appendChild(imgElement);\n",
        "      \n",
        "      const instruction = document.createElement('div');\n",
        "      instruction.innerHTML = \n",
        "          '<span style=\"color: red; font-weight: bold;\">' +\n",
        "          'When finished, click here or on the video to stop this demo</span>';\n",
        "      div.appendChild(instruction);\n",
        "      instruction.onclick = () => { shutdown = true; };\n",
        "      \n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      captureCanvas = document.createElement('canvas');\n",
        "      captureCanvas.width = 640; //video.videoWidth;\n",
        "      captureCanvas.height = 480; //video.videoHeight;\n",
        "      window.requestAnimationFrame(onAnimationFrame);\n",
        "      \n",
        "      return stream;\n",
        "    }\n",
        "    async function stream_frame(label, imgData) {\n",
        "      if (shutdown) {\n",
        "        removeDom();\n",
        "        shutdown = false;\n",
        "        return '';\n",
        "      }\n",
        "\n",
        "      var preCreate = Date.now();\n",
        "      stream = await createDom();\n",
        "      \n",
        "      var preShow = Date.now();\n",
        "      if (label != \"\") {\n",
        "        labelElement.innerHTML = label;\n",
        "      }\n",
        "            \n",
        "      if (imgData != \"\") {\n",
        "        var videoRect = video.getClientRects()[0];\n",
        "        imgElement.style.top = videoRect.top + \"px\";\n",
        "        imgElement.style.left = videoRect.left + \"px\";\n",
        "        imgElement.style.width = videoRect.width + \"px\";\n",
        "        imgElement.style.height = videoRect.height + \"px\";\n",
        "        imgElement.src = imgData;\n",
        "      }\n",
        "      \n",
        "      var preCapture = Date.now();\n",
        "      var result = await new Promise(function(resolve, reject) {\n",
        "        pendingResolve = resolve;\n",
        "      });\n",
        "      shutdown = false;\n",
        "      \n",
        "      return {'create': preShow - preCreate, \n",
        "              'show': preCapture - preShow, \n",
        "              'capture': Date.now() - preCapture,\n",
        "              'img': result};\n",
        "    }\n",
        "    ''')\n",
        "\n",
        "  display(js)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHf1XmpEqZ63"
      },
      "source": [
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SOXwIJ5ly1Z_",
        "outputId": "7dc95012-150b-4891-e649-13d4fb4af367"
      },
      "source": [
        "model = get_model('vgg19')     \n",
        "model.load_state_dict(torch.load('/content/pytorch_Realtime_Multi-Person_Pose_Estimation/pose_model.pth'))\n",
        "model\n",
        "model.float()\n",
        "model.eval()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bulding VGG19\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "rtpose_model(\n",
              "  (model0): Sequential(\n",
              "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (13): ReLU(inplace=True)\n",
              "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (15): ReLU(inplace=True)\n",
              "    (16): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (17): ReLU(inplace=True)\n",
              "    (18): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (19): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (20): ReLU(inplace=True)\n",
              "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (22): ReLU(inplace=True)\n",
              "    (23): Conv2d(512, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (24): ReLU(inplace=True)\n",
              "    (25): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (26): ReLU(inplace=True)\n",
              "  )\n",
              "  (model1_1): Sequential(\n",
              "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(512, 38, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (model2_1): Sequential(\n",
              "    (0): Conv2d(185, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(128, 38, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (model3_1): Sequential(\n",
              "    (0): Conv2d(185, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(128, 38, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (model4_1): Sequential(\n",
              "    (0): Conv2d(185, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(128, 38, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (model5_1): Sequential(\n",
              "    (0): Conv2d(185, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(128, 38, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (model6_1): Sequential(\n",
              "    (0): Conv2d(185, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(128, 38, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (model1_2): Sequential(\n",
              "    (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(512, 19, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (model2_2): Sequential(\n",
              "    (0): Conv2d(185, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(128, 19, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (model3_2): Sequential(\n",
              "    (0): Conv2d(185, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(128, 19, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (model4_2): Sequential(\n",
              "    (0): Conv2d(185, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(128, 19, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (model5_2): Sequential(\n",
              "    (0): Conv2d(185, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(128, 19, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (model6_2): Sequential(\n",
              "    (0): Conv2d(185, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (7): ReLU(inplace=True)\n",
              "    (8): Conv2d(128, 128, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3))\n",
              "    (9): ReLU(inplace=True)\n",
              "    (10): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))\n",
              "    (11): ReLU(inplace=True)\n",
              "    (12): Conv2d(128, 19, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPw7uJ_hjOYl"
      },
      "source": [
        "def video_frame(label, bbox):\n",
        "  data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
        "  return data\n",
        "\n",
        "def js_to_image(js_reply):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          js_reply: JavaScript object containing image from webcam\n",
        "  Returns:\n",
        "          img: OpenCV BGR image\n",
        "  \"\"\"\n",
        "  # decode base64 image\n",
        "  image_bytes = b64decode(js_reply.split(',')[1])\n",
        "  # convert bytes to numpy array\n",
        "  jpg_as_np = np.frombuffer(image_bytes, dtype=np.uint8)\n",
        "  # decode numpy array into OpenCV BGR image\n",
        "  img = cv2.imdecode(jpg_as_np, flags=1)\n",
        "\n",
        "  return img\n",
        "\n",
        "# function to convert OpenCV Rectangle bounding box image into base64 byte string to be overlayed on video stream\n",
        "def bbox_to_bytes(bbox_array):\n",
        "  \"\"\"\n",
        "  Params:\n",
        "          bbox_array: Numpy array (pixels) containing rectangle to overlay on video stream.\n",
        "  Returns:\n",
        "        bytes: Base64 image byte string\n",
        "  \"\"\"\n",
        "  # convert array into PIL image\n",
        "  bbox_PIL = PIL.Image.fromarray(bbox_array, 'RGBA')\n",
        "  iobuf = io.BytesIO()\n",
        "  # format bbox into png for return\n",
        "  bbox_PIL.save(iobuf, format='png')\n",
        "  # format return string\n",
        "  bbox_bytes = 'data:image/png;base64,{}'.format((str(b64encode(iobuf.getvalue()), 'utf-8')))\n",
        "  \n",
        "  return bbox_bytes"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "YEu3fjGeqJfQ",
        "outputId": "1ad09b98-cb54-43a1-a8a7-b0c9de23e459"
      },
      "source": [
        "import time\n",
        "\n",
        "video_stream()\n",
        "\n",
        "# label for video\n",
        "label_html = 'Capturing...'\n",
        "# initialze bounding box to empty\n",
        "bbox = ''\n",
        "count = 0 \n",
        "while True:\n",
        "  js_reply = video_frame(label_html, bbox)\n",
        "  if not js_reply:\n",
        "      break\n",
        "\n",
        "  # convert JS response to OpenCV Image\n",
        "  frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "  # create transparent overlay for bounding box\n",
        "  output = np.zeros([480,640,4], dtype=np.uint8)\n",
        "  a = time.time()\n",
        "  with torch.no_grad():\n",
        "      paf, heatmap, imscale = get_outputs(\n",
        "          frame, model, 'rtpose')\n",
        "  # print(time.time() - a)\n",
        "  bbox_array = paf_to_pose_cpp(heatmap, paf, cfg)\n",
        "  # print(time.time() - a)\n",
        "  # print(\"======================================\")\n",
        "  # print(bbox_array)\n",
        "  # print(bbox_array)\n",
        "  # print(type(bbox_array[0]))\n",
        "  # print(type(bbox_array))\n",
        "  # out = draw_humans(oriImg, humans)\n",
        "  # out = draw_humans(frame, bbox_array)   \n",
        "  # print(out.shape)     \n",
        "  # cv2_imshow(out)\n",
        "  image_h, image_w = output.shape[:2]\n",
        "  centers = {}\n",
        "  for human in bbox_array:\n",
        "      # draw point\n",
        "      for i in range(CocoPart.Background.value):\n",
        "          if i not in human.body_parts.keys():\n",
        "              continue\n",
        "\n",
        "          body_part = human.body_parts[i]\n",
        "          center = (int(body_part.x * image_w + 0.5), int(body_part.y * image_h + 0.5))\n",
        "          centers[i] = center\n",
        "          output = cv2.circle(output, center, 3, CocoColors[i], thickness=3, lineType=8, shift=0)\n",
        "\n",
        "      # draw line\n",
        "      for pair_order, pair in enumerate(CocoPairsRender):\n",
        "          if pair[0] not in human.body_parts.keys() or pair[1] not in human.body_parts.keys():\n",
        "              continue\n",
        "\n",
        "          # npimg = cv2.line(npimg, centers[pair[0]], centers[pair[1]], common.CocoColors[pair_order], 3)\n",
        "          output = cv2.line(output, centers[pair[0]], centers[pair[1]], CocoColors[pair_order], 3)\n",
        "\n",
        "  output[:,:,3] = (output.max(axis = 2) > 0 ).astype(int) * 255\n",
        "  bbox_bytes = bbox_to_bytes(output)\n",
        "  # update bbox so next frame gets new overlay\n",
        "  bbox = bbox_bytes"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    var video;\n",
              "    var div = null;\n",
              "    var stream;\n",
              "    var captureCanvas;\n",
              "    var imgElement;\n",
              "    var labelElement;\n",
              "    \n",
              "    var pendingResolve = null;\n",
              "    var shutdown = false;\n",
              "    \n",
              "    function removeDom() {\n",
              "       stream.getVideoTracks()[0].stop();\n",
              "       video.remove();\n",
              "       div.remove();\n",
              "       video = null;\n",
              "       div = null;\n",
              "       stream = null;\n",
              "       imgElement = null;\n",
              "       captureCanvas = null;\n",
              "       labelElement = null;\n",
              "    }\n",
              "    \n",
              "    function onAnimationFrame() {\n",
              "      if (!shutdown) {\n",
              "        window.requestAnimationFrame(onAnimationFrame);\n",
              "      }\n",
              "      if (pendingResolve) {\n",
              "        var result = \"\";\n",
              "        if (!shutdown) {\n",
              "          captureCanvas.getContext('2d').drawImage(video, 0, 0, 640, 480);\n",
              "          result = captureCanvas.toDataURL('image/jpeg', 0.8)\n",
              "        }\n",
              "        var lp = pendingResolve;\n",
              "        pendingResolve = null;\n",
              "        lp(result);\n",
              "      }\n",
              "    }\n",
              "    \n",
              "    async function createDom() {\n",
              "      if (div !== null) {\n",
              "        return stream;\n",
              "      }\n",
              "\n",
              "      div = document.createElement('div');\n",
              "      div.style.border = '2px solid black';\n",
              "      div.style.padding = '3px';\n",
              "      div.style.width = '100%';\n",
              "      div.style.maxWidth = '600px';\n",
              "      document.body.appendChild(div);\n",
              "      \n",
              "      const modelOut = document.createElement('div');\n",
              "      modelOut.innerHTML = \"<span>Status:</span>\";\n",
              "      labelElement = document.createElement('span');\n",
              "      labelElement.innerText = 'No data';\n",
              "      labelElement.style.fontWeight = 'bold';\n",
              "      modelOut.appendChild(labelElement);\n",
              "      div.appendChild(modelOut);\n",
              "           \n",
              "      video = document.createElement('video');\n",
              "      video.style.display = 'block';\n",
              "      video.width = div.clientWidth - 6;\n",
              "      video.setAttribute('playsinline', '');\n",
              "      video.onclick = () => { shutdown = true; };\n",
              "      stream = await navigator.mediaDevices.getUserMedia(\n",
              "          {video: { facingMode: \"environment\"}});\n",
              "      div.appendChild(video);\n",
              "\n",
              "      imgElement = document.createElement('img');\n",
              "      imgElement.style.position = 'absolute';\n",
              "      imgElement.style.zIndex = 1;\n",
              "      imgElement.onclick = () => { shutdown = true; };\n",
              "      div.appendChild(imgElement);\n",
              "      \n",
              "      const instruction = document.createElement('div');\n",
              "      instruction.innerHTML = \n",
              "          '<span style=\"color: red; font-weight: bold;\">' +\n",
              "          'When finished, click here or on the video to stop this demo</span>';\n",
              "      div.appendChild(instruction);\n",
              "      instruction.onclick = () => { shutdown = true; };\n",
              "      \n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "\n",
              "      captureCanvas = document.createElement('canvas');\n",
              "      captureCanvas.width = 640; //video.videoWidth;\n",
              "      captureCanvas.height = 480; //video.videoHeight;\n",
              "      window.requestAnimationFrame(onAnimationFrame);\n",
              "      \n",
              "      return stream;\n",
              "    }\n",
              "    async function stream_frame(label, imgData) {\n",
              "      if (shutdown) {\n",
              "        removeDom();\n",
              "        shutdown = false;\n",
              "        return '';\n",
              "      }\n",
              "\n",
              "      var preCreate = Date.now();\n",
              "      stream = await createDom();\n",
              "      \n",
              "      var preShow = Date.now();\n",
              "      if (label != \"\") {\n",
              "        labelElement.innerHTML = label;\n",
              "      }\n",
              "            \n",
              "      if (imgData != \"\") {\n",
              "        var videoRect = video.getClientRects()[0];\n",
              "        imgElement.style.top = videoRect.top + \"px\";\n",
              "        imgElement.style.left = videoRect.left + \"px\";\n",
              "        imgElement.style.width = videoRect.width + \"px\";\n",
              "        imgElement.style.height = videoRect.height + \"px\";\n",
              "        imgElement.src = imgData;\n",
              "      }\n",
              "      \n",
              "      var preCapture = Date.now();\n",
              "      var result = await new Promise(function(resolve, reject) {\n",
              "        pendingResolve = resolve;\n",
              "      });\n",
              "      shutdown = false;\n",
              "      \n",
              "      return {'create': preShow - preCreate, \n",
              "              'show': preCapture - preShow, \n",
              "              'capture': Date.now() - preCapture,\n",
              "              'img': result};\n",
              "    }\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhgzUumV8pcP"
      },
      "source": [
        "[BodyPart:0-(0.73, 0.56) score=0.44 BodyPart:14-(0.64, 0.47) score=0.77 BodyPart:15-(0.79, 0.47) score=0.83 BodyPart:16-(0.54, 0.55) score=0.40]\n",
        "<class 'lib.utils.common.Human'>\n",
        "<class 'list'>\n",
        "---------------------------------------------------------------------------\n",
        "AttributeError                            Traceback (most recent call last)\n",
        "<ipython-input-47-42f8c929835e> in <module>()\n",
        "     26   print(type(bbox_array))\n",
        "     27   # out = draw_humans(oriImg, humans)\n",
        "---> 28   bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "     29   # update bbox so next frame gets new overlay\n",
        "     30   bbox = bbox_bytes\n",
        "\n",
        "1 frames\n",
        "/usr/local/lib/python3.7/dist-packages/PIL/Image.py in fromarray(obj, mode)\n",
        "   2702     .. versionadded:: 1.1.6\n",
        "   2703     \"\"\"\n",
        "-> 2704     arr = obj.__array_interface__\n",
        "   2705     shape = arr[\"shape\"]\n",
        "   2706     ndim = len(shape)\n",
        "\n",
        "AttributeError: 'list' object has no attribute '__array_interface__'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mvSIU0mqr0zv",
        "outputId": "bcd0cd33-cc85-4a39-88b3-d9155f8be241"
      },
      "source": [
        "!python /content/test.py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<IPython.core.display.Javascript object>\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/test.py\", line 191, in <module>\n",
            "    js_reply = video_frame(label_html, bbox)\n",
            "  File \"/content/test.py\", line 144, in video_frame\n",
            "    data = eval_js('stream_frame(\"{}\", \"{}\")'.format(label, bbox))\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/colab/output/_js.py\", line 36, in eval_js\n",
            "    kernel = _ipython.get_kernel()\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/google/colab/_ipython.py\", line 28, in get_kernel\n",
            "    return get_ipython().kernel\n",
            "AttributeError: 'InteractiveShell' object has no attribute 'kernel'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 200
        },
        "id": "O2bHhnqGLwDw",
        "outputId": "41718e6a-bb8b-4775-a4f8-244a9def92d4"
      },
      "source": [
        "\n",
        "\n",
        "  \n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--cfg', help='experiment configure file name',\n",
        "                    default='./experiments/vgg19_368x368_sgd.yaml', type=str)\n",
        "parser.add_argument('--weight', type=str,\n",
        "                    default='pose_model.pth')\n",
        "# parser.add_argument('opts',\n",
        "#                     help=\"Modify config options using the command-line\",\n",
        "#                     default=None,\n",
        "#                     nargs=argparse.REMAINDER)\n",
        "# args = parser.parse_args()\n",
        "\n",
        "# update config file\n",
        "# update_config('/content/pytorch_Realtime_Multi-Person_Pose_Estimation/experiments/vgg19_368x368_sgd.yaml', Namespace(cfg='./experiments/vgg19_368x368_sgd.yaml', opts=[], weight='pose_model.pth'))   \n",
        "\n",
        "model = get_model('vgg19')     \n",
        "model.load_state_dict(torch.load(args.weight))\n",
        "model.cuda()\n",
        "model.float()\n",
        "model.eval()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    # video_capture = cv2.VideoCapture(0)\n",
        "    # print(video_capture)\n",
        "\n",
        "\n",
        "    video_stream()\n",
        "    # label for video\n",
        "    label_html = 'Capturing...'\n",
        "    # initialze bounding box to empty\n",
        "    bbox = ''\n",
        "    count = 0 \n",
        "    while True:\n",
        "        js_reply = video_frame(label_html, bbox)\n",
        "        if not js_reply:\n",
        "            break\n",
        "\n",
        "        # convert JS response to OpenCV Image\n",
        "        frame = js_to_image(js_reply[\"img\"])\n",
        "\n",
        "        # create transparent overlay for bounding box\n",
        "        bbox_array = np.zeros([480,640,4], dtype=np.uint8)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            paf, heatmap, imscale = get_outputs(\n",
        "                oriImg, model, 'rtpose')\n",
        "\n",
        "        bbox_array = paf_to_pose_cpp(heatmap, paf, cfg)\n",
        "                \n",
        "        # out = draw_humans(oriImg, humans)        \n",
        "        bbox_bytes = bbox_to_bytes(bbox_array)\n",
        "        # update bbox so next frame gets new overlay\n",
        "        bbox = bbox_bytes\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "usage: ipykernel_launcher.py [-h] [--cfg CFG] [--weight WEIGHT]\n",
            "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-72117f84-270f-4d14-b223-fef759ebddbb.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "ignored",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py:2890: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7x3vtnXdyMf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}